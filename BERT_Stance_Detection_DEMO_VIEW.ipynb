{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT Stance Detection DEMO VIEW",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mcsantiago/BERT-COVID-StanceDetection/blob/main/BERT_Stance_Detection_DEMO_VIEW.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MEyHIpGQ736C"
      },
      "source": [
        "# Stance BERTection :D"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZiUtVEYtYt8"
      },
      "source": [
        "## TODO:\n",
        "- Regenerate the 0.4 drop model graph\n",
        "- Figure out how to load weights\n",
        "- Demo only interface\n",
        "- Figure out how to incorporate other embeddings from other models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMIFXX95DHrd"
      },
      "source": [
        "## Download the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ziKrk2W0iBY9",
        "outputId": "08f26208-172b-4732-b173-508fb261f11e"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWUH7NdMHGEK"
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "import re\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zsa9-FuTHN-R",
        "outputId": "50765896-ae7c-411e-b333-b169442c081a"
      },
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "1NEwaH-zDiX5",
        "outputId": "aece2fde-a22c-4301-9a2a-ecc7beec810a"
      },
      "source": [
        "# Setup stopwords list & word (noun, adjective, and verb) lemmatizer\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"Function to clean text using RegEx operations, removal of stopwords, and lemmatization.\"\"\"\n",
        "    text = re.sub(r'[^\\w\\s]', '', text, re.UNICODE)\n",
        "    text = text.lower()\n",
        "    text = [lemmatizer.lemmatize(token) for token in text.split(' ')]\n",
        "    text = [lemmatizer.lemmatize(token, 'v') for token in text]\n",
        "    text = [word for word in text if word not in stop_words]\n",
        "    text = ' '.join(text)\n",
        "    text = text.lstrip().rstrip()\n",
        "    text = re.sub(' +', ' ', text)\n",
        "    return text\n",
        "\n",
        "clean_text('the cat is trained for the #lemmatization of idk.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'cat train lemmatization idk'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YikvGoScfiiz"
      },
      "source": [
        "import logging\n",
        "\n",
        "# Disable unwanted warning messages from pytorch_transformers\n",
        "# NOTE: Run once without the line below to check if anything is wrong, here we target to eliminate\n",
        "# the message \"Token indices sequence length is longer than the specified maximum sequence length\"\n",
        "# since we already take care of it within the tokenize() function through fixing sequence length\n",
        "logging.getLogger('pytorch_transformers').setLevel(logging.CRITICAL)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n3EquDMbRHcV",
        "outputId": "16128a1b-27a3-4c4c-c0bd-a12f3fc3e9ad"
      },
      "source": [
        "!pip install pytorch_transformers # We will need this for our model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pytorch_transformers in /usr/local/lib/python3.7/dist-packages (1.2.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch_transformers) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pytorch_transformers) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch_transformers) (1.19.5)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from pytorch_transformers) (0.0.45)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from pytorch_transformers) (2019.12.20)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_transformers) (1.8.1+cu101)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from pytorch_transformers) (0.1.95)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.7/dist-packages (from pytorch_transformers) (1.17.62)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_transformers) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_transformers) (3.0.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->pytorch_transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->pytorch_transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->pytorch_transformers) (1.0.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.0.0->pytorch_transformers) (3.7.4.3)\n",
            "Requirement already satisfied: botocore<1.21.0,>=1.20.62 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch_transformers) (1.20.62)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch_transformers) (0.10.0)\n",
            "Requirement already satisfied: s3transfer<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch_transformers) (0.4.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.21.0,>=1.20.62->boto3->pytorch_transformers) (2.8.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RsTCVtZmfrbi",
        "outputId": "cfa8280d-70a9-4b03-c40f-1fead74bb5f6"
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from pytorch_transformers import BertConfig, BertTokenizer, BertModel\n",
        "\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"DEVICE FOUND: %s\" % DEVICE)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DEVICE FOUND: cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LI7hT5t4gBVv"
      },
      "source": [
        "# Set seeds for reproducibility\n",
        "SEED = 42\n",
        "torch.manual_seed(seed=SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3N63y-17ksGs"
      },
      "source": [
        "## Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mzK9F-dYgEAp"
      },
      "source": [
        "# Define hyperparameters\n",
        "NUM_EPOCHS = 20\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "PRETRAINED_MODEL_NAME = 'bert-base-cased'\n",
        "NUM_PRETRAINED_BERT_LAYERS = 1 # 2 -> 1\n",
        "MAX_TOKENIZATION_LENGTH = 512\n",
        "NUM_CLASSES = 4 # agree, disagree, no_stance, not_relevant\n",
        "TOP_DOWN = True\n",
        "NUM_RECURRENT_LAYERS = 1 # 2 -> 1\n",
        "HIDDEN_SIZE = 256\n",
        "REINITIALIZE_POOLER_PARAMETERS = True\n",
        "USE_BIDIRECTIONAL = True\n",
        "DROPOUT_RATE = 0.6\n",
        "AGGREGATE_ON_CLS_TOKEN = True\n",
        "CONCATENATE_HIDDEN_STATES = True\n",
        "\n",
        "APPLY_CLEANING = True\n",
        "TRUNCATION_METHOD = 'head-only'\n",
        "NUM_WORKERS = 0\n",
        "\n",
        "BERT_LEARNING_RATE = 1e-5\n",
        "CUSTOM_LEARNING_RATE = 1e-4\n",
        "BETAS = (0.9, 0.999)\n",
        "BERT_WEIGHT_DECAY = 0.01\n",
        "EPS = 1e-8\n",
        "\n",
        "# Regularization\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L11timVJih3P"
      },
      "source": [
        "## FineTunedBert"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fg3kSUr7vQIZ"
      },
      "source": [
        "class FineTunedBert(nn.Module):\n",
        "    \"\"\"\n",
        "    Finetuning model that utilizes BERT tokenizer, pretrained BERT embedding, pretrained BERT\n",
        "    encoders, an optional recurrent neural network  choice of LSTM, dropout, and finally a dense\n",
        "    layer for classification.\n",
        "    @param (str) pretrained_model_name: name of the pretrained BERT model for tokenizing input\n",
        "           sequences, extracting vector representations for each token, [...]\n",
        "    @param (int) num_pretrained_bert_layers: number of BERT Encoder layers to be utilized\n",
        "    @param (int) max_tokenization_length: maximum number of positional embeddings, or the sequence\n",
        "           length of an example that will be fed to BERT model (default: 512)\n",
        "    @param (int) num_classes: number of classes to distinct between for classification; specify\n",
        "           2 for binary classification (default: 1)\n",
        "    @param (bool) top_down: whether to assign parameters (weights and biases) in order or\n",
        "           backwards (default: True)\n",
        "    @param (int) num_recurrent_layers: number of LSTM layers to utilize (default: 1)\n",
        "    @param (bool) use_bidirectional: whether to use a bidirectional LSTM or not (default: False)\n",
        "    @param (int) hidden_size: number of recurrent units in each LSTM cell (default: 128)\n",
        "    @param (bool) reinitialize_pooler_parameters: whether to use the pretrained pooler parameters\n",
        "           or initialize weights as ones and biases zeros and train for scratch (default: False)\n",
        "    @param (float) dropout_rate: possibility of each neuron to be discarded (default: 0.10)\n",
        "    @param (bool) aggregate_on_cls_token: whether to pool on only the hidden states of the [CLS]\n",
        "           token for classification or on the hidden states of all (512) tokens (default: True)\n",
        "    @param (bool) concatenate_hidden_states: whether to concatenate all the available hidden states\n",
        "           outputted by the embedding and encoder layers (K+1) or only use the latest hidden state\n",
        "           (default: False)\n",
        "    @param (bool) use_gpu: whether to utilize GPU (CUDA) or not (default: False)\n",
        "    \"\"\"\n",
        "    def __init__(self, pretrained_model_name, num_pretrained_bert_layers, max_tokenization_length, vocab_file = '',\n",
        "                 num_classes=1, top_down=True, num_recurrent_layers=1, use_bidirectional=False,\n",
        "                 hidden_size=128, reinitialize_pooler_parameters=False, dropout_rate=0.10,\n",
        "                 aggregate_on_cls_token=True, concatenate_hidden_states=False, use_gpu=False):\n",
        "        super(FineTunedBert, self).__init__()\n",
        "        self.num_recurrent_layers = num_recurrent_layers\n",
        "        self.use_bidirectional = use_bidirectional\n",
        "        self.hidden_size = hidden_size\n",
        "        self.aggregate_on_cls_token = aggregate_on_cls_token\n",
        "        self.concatenate_hidden_states = concatenate_hidden_states\n",
        "        self.use_gpu = use_gpu\n",
        "\n",
        "        # Configure tokenizer\n",
        "        self.tokenizer = BertTokenizer.from_pretrained(pretrained_model_name)\n",
        "        self.tokenizer.max_len = max_tokenization_length\n",
        "        self.tokenizer.vocab_file = vocab_file\n",
        "\n",
        "        # Get global BERT config\n",
        "        self.config = BertConfig.from_pretrained(pretrained_model_name)\n",
        "        # Extract all parameters (weights and bias matrices) for the 12 layers\n",
        "        all_states_dict = BertModel.from_pretrained(pretrained_model_name,\n",
        "                                                    config=self.config).state_dict()\n",
        "\n",
        "        # Get customized BERT config\n",
        "        self.config.max_position_embeddings = max_tokenization_length\n",
        "        self.config.num_hidden_layers = num_pretrained_bert_layers\n",
        "        self.config.output_hidden_states = True\n",
        "        self.config.output_attentions = True\n",
        "\n",
        "        # Get pretrained BERT model & all its learnable parameters\n",
        "        self.bert = BertModel.from_pretrained(pretrained_model_name,\n",
        "                                              config=self.config)\n",
        "        current_states_dict = self.bert.state_dict()\n",
        "\n",
        "        # Assign matching parameters (weights and biases of all kinds of layers)\n",
        "        # i)  Top-Down Approach: 1st layer takes weights of 1st pretrained BERT layer\n",
        "        if top_down:\n",
        "            for param in current_states_dict.keys():\n",
        "                if 'pooler' not in param or not reinitialize_pooler_parameters:\n",
        "                    current_states_dict[param] = all_states_dict[param]\n",
        "                else:\n",
        "                    if 'weight' in param:\n",
        "                        current_states_dict[param] = torch.ones(self.config.hidden_size,\n",
        "                                                                self.config.hidden_size)\n",
        "                    elif 'bias' in param:\n",
        "                        current_states_dict[param] = torch.zeros(self.config.hidden_size)\n",
        "\n",
        "        # ii) Bottom-Up Approach: 1st layer takes weights of 12th (last) pretrained BERT layer\n",
        "        else:\n",
        "            align = 5 + ((12 - num_pretrained_bert_layers) * 16)\n",
        "            for index, param in enumerate(current_states_dict.keys()):\n",
        "                # There are 5 initial (shared) parameters from embeddings in each BERT model\n",
        "                if index < 5 and 'embeddings' in param:\n",
        "                    current_states_dict[param] = all_states_dict[param]\n",
        "                # There are 16 parameters for each of the K pretrained BERT layers (16 x K params)\n",
        "                elif index >= 5 and 'pooler' not in param:\n",
        "                    current_states_dict[param] = list(all_states_dict.values())[align:][index-5]\n",
        "                # There are 2 parameters for the pooling layer at the end in each BERT model\n",
        "                else:\n",
        "                    if not reinitialize_pooler_parameters:\n",
        "                        current_states_dict[param] = all_states_dict[param]\n",
        "                    else:\n",
        "                        if 'weight' in param:\n",
        "                            current_states_dict[param] = torch.ones(self.config.hidden_size,\n",
        "                                                                    self.config.hidden_size)\n",
        "                        elif 'bias' in param:\n",
        "                            current_states_dict[param] = torch.zeros(self.config.hidden_size)\n",
        "\n",
        "        del all_states_dict\n",
        "        # Update parameters in extracted BERT model\n",
        "        self.bert.load_state_dict(current_states_dict)\n",
        "\n",
        "        logging.info('Loaded %d learnable parameters from pretrained BERT model with %d layer(s)' %\n",
        "                     (len(list(self.bert.parameters())), num_pretrained_bert_layers))\n",
        "\n",
        "        # Number of input hidden dimensions from the final BERT layer, as input to other layers\n",
        "        input_hidden_dimension = None\n",
        "        if concatenate_hidden_states:\n",
        "            input_hidden_dimension = (num_pretrained_bert_layers + 1) * self.config.hidden_size\n",
        "        else:\n",
        "            input_hidden_dimension = self.config.hidden_size\n",
        "\n",
        "        # Define additional layers & utilities specific to the finetuned task\n",
        "        # Flatten tensors to (B, P*(H or H')) -> converts tensors to 2D for classification\n",
        "        self.flatten_sequence_length = lambda t: t.view(-1,\n",
        "                                                        self.config.max_position_embeddings *\n",
        "                                                        input_hidden_dimension)\n",
        "\n",
        "        # Dropout to prevent overfitting\n",
        "        self.dropout = nn.Dropout(p=dropout_rate)\n",
        "        if self.num_recurrent_layers > 0:\n",
        "            # Recurrent Layer\n",
        "            self.lstm = nn.LSTM(input_size=input_hidden_dimension,\n",
        "                                hidden_size=hidden_size,\n",
        "                                num_layers=num_recurrent_layers,\n",
        "                                bidirectional=use_bidirectional,\n",
        "                                batch_first=True)\n",
        "            # Dense Layer for Classification\n",
        "            self.clf = nn.Linear(in_features=hidden_size*2 if use_bidirectional else hidden_size,\n",
        "                                 out_features=num_classes)\n",
        "        else:\n",
        "            # Dense Layer for Classification\n",
        "            if aggregate_on_cls_token:\n",
        "                self.clf = nn.Linear(in_features=input_hidden_dimension,\n",
        "                                     out_features=num_classes)\n",
        "            else:\n",
        "                self.clf = nn.Linear(in_features=max_tokenization_length * input_hidden_dimension,\n",
        "                                     out_features=num_classes)\n",
        "\n",
        "    def get_tokenizer(self):\n",
        "        \"\"\"Function to easily access the BERT tokenizer\"\"\"\n",
        "        return self.tokenizer\n",
        "\n",
        "    def get_bert_attention(self, raw_sentence, header, device):\n",
        "        \"\"\"Function for getting the multi-head self-attention output from pretrained BERT\"\"\"\n",
        "        # Tokenize & encode raw sentence\n",
        "        x = tokenize_and_encode(text=raw_sentence,                             # (P)\n",
        "                                header=header,\n",
        "                                tokenizer=self.get_tokenizer(),\n",
        "                                max_tokenization_length=self.config.max_position_embeddings,\n",
        "                                truncation_method='head-only')\n",
        "        # Convert the tokenized list to a Tensor\n",
        "        x = torch.tensor(data=x, device=device)\n",
        "        # Reshape input for BERT output\n",
        "        x = x.unsqueeze(dim=1).view(1, -1)                                     # (B=1, P)\n",
        "\n",
        "        # Get features\n",
        "        token_type_ids, attention_mask = get_features(input_ids=x,\n",
        "                                                      tokenizer=self.get_tokenizer(),\n",
        "                                                      device=device)\n",
        "        # Pass tokenized sequence through pretrained BERT model\n",
        "        bert_outputs = self.bert(input_ids=x,                                  # (...) SEE forward()\n",
        "                                 token_type_ids=token_type_ids,\n",
        "                                 attention_mask=attention_mask,\n",
        "                                 position_ids=None,\n",
        "                                 head_mask=None)\n",
        "        attention_outputs = bert_outputs[3]                                    # ([K] x (1, N, P, P))\n",
        "        return attention_outputs\n",
        "\n",
        "    def predict(self, text, header_id):\n",
        "        # get input_ids (tokenize_and_encode)\n",
        "        header = targets[target_idx.index(header_id)]\n",
        "        stance_input_ids = tokenize_and_encode(text, \n",
        "                                               self.get_tokenizer(), \n",
        "                                               header, \n",
        "                                               apply_cleaning=APPLY_CLEANING,\n",
        "                                               max_tokenization_length=MAX_TOKENIZATION_LENGTH,\n",
        "                                               truncation_method=TRUNCATION_METHOD)\n",
        "        # Batch size of 1\n",
        "        stance_input_ids = torch.from_numpy(np.array(stance_input_ids).reshape(1, -1)).long().to(DEVICE)\n",
        "        # get attention_mask / token_id (get_features)\n",
        "        stance_token_id, stance_attention_mask = get_features(stance_input_ids,\n",
        "                                                              tokenizer=self.get_tokenizer(),\n",
        "                                                              device=DEVICE)\n",
        "        # forward\n",
        "        return self.forward(input_ids=stance_input_ids,\n",
        "                            token_type_ids=stance_token_id,\n",
        "                            attention_mask=stance_attention_mask)\n",
        "  \n",
        "    def forward(self, input_ids, token_type_ids=None, attention_mask=None,     # input_ids: (B, P)\n",
        "                position_ids=None, head_mask=None):\n",
        "        \"\"\"Function implementing a forward pass of the model\"\"\"\n",
        "        # Pass tokenized sequence through pretrained BERT model\n",
        "        bert_outputs = self.bert(input_ids=input_ids,\n",
        "                                 token_type_ids=token_type_ids,\n",
        "                                 attention_mask=attention_mask,\n",
        "                                 position_ids=position_ids,\n",
        "                                 head_mask=head_mask)\n",
        "        sequence_output = bert_outputs[0]                                      # (B, P, H)\n",
        "        pooled_output = bert_outputs[1]                                        # (B, H)\n",
        "        hidden_outputs = bert_outputs[2]                                       # ([K+1] x (B, P, H))\n",
        "        attention_outputs = bert_outputs[3]                                    # ([K] x (B, N, P, P))\n",
        "\n",
        "        if self.concatenate_hidden_states:\n",
        "            sequence_output = torch.cat(hidden_outputs, dim=-1)                # (B, P, H' = (K+1) x H)\n",
        "\n",
        "        if self.num_recurrent_layers > 0:\n",
        "            # Set initial states\n",
        "            if self.use_gpu:\n",
        "                h0 = Variable(torch.zeros(self.num_recurrent_layers * 2        # (L * 2 OR L, B, H)\n",
        "                                          if self.use_bidirectional else self.num_recurrent_layers,\n",
        "                                          input_ids.shape[0],\n",
        "                                          self.hidden_size)).cuda()\n",
        "                c0 = Variable(torch.zeros(self.num_recurrent_layers * 2        # (L * 2 OR L, B, H)\n",
        "                                          if self.use_bidirectional else self.num_recurrent_layers,\n",
        "                                          input_ids.shape[0],\n",
        "                                          self.hidden_size)).cuda()\n",
        "            else:\n",
        "                h0 = Variable(torch.zeros(self.num_recurrent_layers * 2        # (L * 2 OR L, B, H)\n",
        "                                          if self.use_bidirectional else self.num_recurrent_layers,\n",
        "                                          input_ids.shape[0],\n",
        "                                          self.hidden_size))\n",
        "                c0 = Variable(torch.zeros(self.num_recurrent_layers * 2        # (L * 2 OR L, B, H)\n",
        "                                          if self.use_bidirectional else self.num_recurrent_layers,\n",
        "                                          input_ids.shape[0],\n",
        "                                          self.hidden_size))\n",
        "\n",
        "            lstm_output = self.lstm(sequence_output, (h0, c0))                 # (B, P, H*), (2 x (B, B, H*))\n",
        "            sequence_output, _ = lstm_output\n",
        "\n",
        "            # Get last timesteps for each example in the batch; we do this to counteract padding\n",
        "            last_timesteps = []\n",
        "            for i in range(len(attention_mask)):\n",
        "                last_timesteps.append(\n",
        "                    attention_mask[i].tolist().index(0)\n",
        "                    if 0 in attention_mask[i].tolist() else self.tokenizer.max_len - 1\n",
        "                )\n",
        "\n",
        "            if self.use_gpu:\n",
        "                last_timesteps = torch.tensor(data=last_timesteps).cuda()      # (B)\n",
        "            else:\n",
        "                last_timesteps = torch.tensor(data=last_timesteps)             # (B)\n",
        "            relative_hidden_size = self.hidden_size*2 if self.use_bidirectional else self.hidden_size\n",
        "            last_timesteps = last_timesteps.repeat(1, relative_hidden_size)    # (1, B x H*)\n",
        "            last_timesteps = last_timesteps.view(-1, 1, relative_hidden_size)  # (B, 1, H*)\n",
        "            pooled_sequence_output = sequence_output.gather(                   # (B, H*)\n",
        "                dim=1,\n",
        "                index=last_timesteps\n",
        "            ).squeeze()\n",
        "\n",
        "            pooled_sequence_output = self.dropout(pooled_sequence_output)      # (B, H*)\n",
        "            logits = self.clf(pooled_sequence_output)                          # (B, num_classes)\n",
        "        else:\n",
        "            if not self.aggregate_on_cls_token:\n",
        "                pooled_output = self.flatten_sequence_length(sequence_output)  # (B, P x H)\n",
        "\n",
        "            pooled_output = self.dropout(pooled_output)                        # (B, P x H OR H)\n",
        "            logits = self.clf(pooled_output)                                   # (B, num_classes)\n",
        "\n",
        "        return logits                                                          # (B, num_classes)\n",
        "\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxCTvXjTgG59"
      },
      "source": [
        "# Initialize to-be-finetuned Bert model\n",
        "model = FineTunedBert(pretrained_model_name=PRETRAINED_MODEL_NAME,\n",
        "                      vocab_file=\"drive/MyDrive/CS 6320 Project/data/vocabulary.txt\",\n",
        "                      num_pretrained_bert_layers=NUM_PRETRAINED_BERT_LAYERS,\n",
        "                      max_tokenization_length=MAX_TOKENIZATION_LENGTH,\n",
        "                      num_classes=NUM_CLASSES,\n",
        "                      top_down=TOP_DOWN,\n",
        "                      num_recurrent_layers=NUM_RECURRENT_LAYERS,\n",
        "                      use_bidirectional=USE_BIDIRECTIONAL,\n",
        "                      hidden_size=HIDDEN_SIZE,\n",
        "                      reinitialize_pooler_parameters=REINITIALIZE_POOLER_PARAMETERS,\n",
        "                      dropout_rate=DROPOUT_RATE,\n",
        "                      aggregate_on_cls_token=AGGREGATE_ON_CLS_TOKEN,\n",
        "                      concatenate_hidden_states=CONCATENATE_HIDDEN_STATES,\n",
        "                      use_gpu=True if torch.cuda.is_available() else False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zbG1gFLUjL6b"
      },
      "source": [
        "## tokenize_and_encode"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5dBeLwOujdTK"
      },
      "source": [
        "targets = [\"RNA alters a person's DNA when taking the COVID-19 vaccine.\",\n",
        "            \"The COVID-19 vaccine causes infertility or miscarriages in women.\",\n",
        "            \"Natural COVID-19 immunity is better than immunity derived from a COVID-19 vaccine.\",\n",
        "            \"The COVID-19 vaccine causes Bell's palsy.\",\n",
        "            \"The COVID-19 vaccine contains tissue from aborted fetuses.\",\n",
        "            \"The COVID-19 vaccine was developed to control the general population either through microchip tracking or nanotransducers in our brains.\",\n",
        "            \"More people will die as a result of a negative side effect to the COVID-19 vaccine than would actually die from the coronavirus.\",\n",
        "            \"There are severe side effects of the coronavirus vaccines, worse than having the virus.\"]\n",
        "target_idx = [1,2,3,4,7,8,9,10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PI9Tu71piwDn"
      },
      "source": [
        "import os\n",
        "import re\n",
        "from torch.utils.data import Dataset\n",
        "from tqdm import trange\n",
        "\n",
        "def tokenize_and_encode(text, tokenizer, header='', apply_cleaning=False, max_tokenization_length=512,\n",
        "                        truncation_method='head-only', split_head_density=0.5):\n",
        "    \"\"\"\n",
        "    Function to tokenize & encode a given text.\n",
        "    @param (str) text: a sequence of words to be tokenized in raw string format\n",
        "    @param (pytorch_transformers.BertTokenizer) tokenizer: tokenizer with pre-figured mappings\n",
        "    @param (bool) apply_cleaning: whether or not to perform common cleaning operations on texts;\n",
        "           note that enabling only makes sense if language of the task is English (default: False)\n",
        "    @param (int) max_tokenization_length: maximum number of positional embeddings, or the sequence\n",
        "           length of an example that will be fed to BERT model (default: 512)\n",
        "    @param (str) truncation_method: method that will be applied in case the text exceeds\n",
        "           @max_tokenization_length; currently implemented methods include 'head-only', 'tail-only',\n",
        "           and 'head+tail' (default: 'head-only')\n",
        "    @param (float) split_head_density: weight on head when splitting between head and tail, only\n",
        "           applicable if @truncation_method='head+tail' (default: 0.5)\n",
        "    @return (list) input_ids: the encoded integer indexes of the given text; note that\n",
        "            get_data_iterators() function converts this to a Tensor under the hood\n",
        "    \"\"\"\n",
        "    if apply_cleaning:\n",
        "        text = clean_text(text=text)\n",
        "\n",
        "    # print(text, header)\n",
        "    # Tokenize and encode\n",
        "    tokenized_text = tokenizer.tokenize(text)\n",
        "    input_ids = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
        "    tokenized_header = tokenizer.tokenize(header)\n",
        "    header_ids = tokenizer.convert_tokens_to_ids(tokenized_header)\n",
        "\n",
        "    # Subtract 2 ([CLS] and[SEP] tokens) to get the actual text tokenization length\n",
        "    text_tokenization_length = max_tokenization_length - 3 - len(header_ids)\n",
        "    # Truncate sequences with the specified approach\n",
        "    if len(input_ids) > text_tokenization_length:\n",
        "        # i)   Head-Only Approach: Keep the first N tokens\n",
        "        if truncation_method == 'head-only':\n",
        "            input_ids = input_ids[:text_tokenization_length]\n",
        "        # ii)  Tail-Only Approach: Keep the last N tokens\n",
        "        elif truncation_method == 'tail-only':\n",
        "            input_ids = input_ids[-text_tokenization_length:]\n",
        "        # iii) Head+Tail Approach: Keep the first F tokens and last L tokens where F + L = N\n",
        "        elif truncation_method == 'head+tail':\n",
        "            head_tokenization_length = int(text_tokenization_length * split_head_density)\n",
        "            tail_tokenization_length = text_tokenization_length - head_tokenization_length\n",
        "            input_head_ids = input_ids[:head_tokenization_length]\n",
        "            input_tail_ids = input_ids[-tail_tokenization_length:]\n",
        "            input_ids = input_head_ids + input_tail_ids  \n",
        "\n",
        "    # Plug in CLS & SEP special tokens for identification of start & end points of sequences\n",
        "    cls_id = tokenizer.convert_tokens_to_ids('[CLS]')\n",
        "    sep_id = tokenizer.convert_tokens_to_ids('[SEP]')\n",
        "    input_ids = [cls_id] + header_ids + [sep_id] + input_ids + [sep_id]\n",
        "    #input_ids = tokenizer.encode(input_ids, )\n",
        "\n",
        "\n",
        "    # Pad sequences & corresponding masks and features\n",
        "    pad_id = tokenizer.convert_tokens_to_ids('[PAD]')\n",
        "    if len(input_ids) < max_tokenization_length:\n",
        "        padding_length = max_tokenization_length - len(input_ids)\n",
        "        input_ids = input_ids + ([pad_id] * padding_length)\n",
        "\n",
        "    # Check if input is in correct length\n",
        "    # assert len(input_ids) == max_tokenization_length\n",
        "    return input_ids\n",
        "\n",
        "\n",
        "def get_features(input_ids, tokenizer, device):\n",
        "    \"\"\"\n",
        "    Function to get BERT-related features, and helps to build the total input representation.\n",
        "    @param (Tensor) input_ids: the encoded integer indexes of a batch, with shape: (B, P)\n",
        "    @param (Tensor) header_ids: the encoded integer indexes of a batch, with shape: (B, P)\n",
        "    @param (pytorch_transformers.BertTokenizer) tokenizer: tokenizer with pre-figured mappings\n",
        "    @param (torch.device) device: 'cpu' or 'gpu', decides where to store the outputted tensors\n",
        "    @return (Tensor, Tensor) token_type_ids, attention_mask: features describe token type with\n",
        "            a 0 for the first sentence and a 1 for the pair sentence; enable attention on a\n",
        "            particular token with a 1 or disable it with a 0\n",
        "    \"\"\"\n",
        "    token_type_ids, attention_mask = [], []\n",
        "\n",
        "    # Iterate over batch\n",
        "    for input_ids_example in input_ids:\n",
        "        # Convert tensor to a 1D list\n",
        "        input_ids_example = input_ids_example.squeeze().tolist()\n",
        "        # Set example to whole input when batch size is 1\n",
        "        if input_ids.shape[0] == 1:\n",
        "            input_ids_example = input_ids.squeeze().tolist()\n",
        "        # Get padding information\n",
        "        padding_token_id = tokenizer.convert_tokens_to_ids('[PAD]')\n",
        "        padding_length = input_ids_example.count(padding_token_id)\n",
        "        text_length = len(input_ids_example) - padding_length\n",
        "\n",
        "        # Get segment IDs -> all 0s for one sentence, which is the case for sequence classification\n",
        "        token_type_ids_example = [0] * len(input_ids_example)\n",
        "        # Get input mask -> 1 for real tokens, 0 for padding tokens\n",
        "        attention_mask_example = ([1] * text_length) + ([0] * padding_length)\n",
        "\n",
        "        # Check if features are in correct length\n",
        "        assert len(token_type_ids_example) == len(input_ids_example)\n",
        "        assert len(attention_mask_example) == len(input_ids_example)\n",
        "        token_type_ids.append(token_type_ids_example)\n",
        "        attention_mask.append(attention_mask_example)\n",
        "\n",
        "    # Convert lists to tensors\n",
        "    token_type_ids = torch.tensor(data=token_type_ids, device=device)\n",
        "    attention_mask = torch.tensor(data=attention_mask, device=device)\n",
        "    return token_type_ids, attention_mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHVdHBTAj5f0"
      },
      "source": [
        "## TweetDataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0TlBFBBhfsO"
      },
      "source": [
        "\n",
        "class TweetDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Tweet Dataset for easily iterating over and performing common operations.\n",
        "    @param (str) input_directory: path of directory where the desired data exists\n",
        "    @param (pytorch_transformers.BertTokenizer) tokenizer: tokenizer with pre-figured mappings\n",
        "    @param (bool) apply_cleaning: whether or not to perform common cleaning operations on texts;\n",
        "           note that enabling only makes sense if language of the task is English\n",
        "    @param (int) max_tokenization_length: maximum number of positional embeddings, or the sequence\n",
        "           length of an example that will be fed to BERT model (default: 512)\n",
        "    @param (str) truncation_method: method that will be applied in case the text exceeds\n",
        "           @max_tokenization_length; currently implemented methods include 'head-only', 'tail-only',\n",
        "           and 'head+tail' (default: 'head-only')\n",
        "    @param (float) split_head_density: weight on head when splitting between head and tail, only\n",
        "           applicable if @truncation_method='head+tail' (default: 0.5)\n",
        "    @param (torch.device) device: 'cpu' or 'gpu', decides where to store the data tensors\n",
        "    \"\"\"\n",
        "    def __init__(self, input_directory, tokenizer, apply_cleaning, max_tokenization_length,\n",
        "                 truncation_method='head-only', split_head_density=0.5, device='cpu'):\n",
        "        super(TweetDataset).__init__()\n",
        "        # agree folder\n",
        "        self.agree_path = os.path.join(input_directory, 'agree')\n",
        "        self.agree_files = [f for f in os.listdir(self.agree_path)\n",
        "                               if os.path.isfile(os.path.join(self.agree_path, f))]\n",
        "        self.num_agree_examples = len(self.agree_files)\n",
        "        self.agree_label = 0\n",
        "        # disagree folder\n",
        "        self.disagree_path = os.path.join(input_directory, 'disagree')\n",
        "        self.disagree_files = [f for f in os.listdir(self.disagree_path)\n",
        "                               if os.path.isfile(os.path.join(self.disagree_path, f))]\n",
        "        self.num_disagree_examples = len(self.disagree_files)\n",
        "        self.disagree_label = 1\n",
        "        # nostance folder\n",
        "        self.nostance_path = os.path.join(input_directory, 'no_stance')\n",
        "        self.nostance_files = [f for f in os.listdir(self.nostance_path)\n",
        "                               if os.path.isfile(os.path.join(self.nostance_path, f))]\n",
        "        self.num_nostance_examples = len(self.nostance_files)\n",
        "        self.nostance_label = 2\n",
        "        # notrelevant folder\n",
        "        self.notrelevant_path = os.path.join(input_directory, 'not_relevant')\n",
        "        self.notrelevant_files = [f for f in os.listdir(self.notrelevant_path)\n",
        "                               if os.path.isfile(os.path.join(self.notrelevant_path, f))]\n",
        "        self.num_notrelevant_examples = len(self.notrelevant_files)\n",
        "        self.notrelevant_label = 3\n",
        "\n",
        "        self.tokenizer = tokenizer\n",
        "        self.apply_cleaning = apply_cleaning\n",
        "        self.max_tokenization_length = max_tokenization_length\n",
        "        self.truncation_method = truncation_method\n",
        "        self.split_head_density = split_head_density\n",
        "        self.device = device\n",
        "\n",
        "        # Pre-tokenize & encode examples\n",
        "        self.pre_tokenize_and_encode_examples()\n",
        "\n",
        "    def pre_tokenize_and_encode_examples(self):\n",
        "        \"\"\"\n",
        "        Function to tokenize & encode examples and save the tokenized versions to a separate folder.\n",
        "        This way, we won't have to perform the same tokenization and encoding ops every epoch.\n",
        "        \"\"\"\n",
        "        self.__pre_tokenize_and_encode_examples__(self.agree_path, self.agree_files)\n",
        "        self.__pre_tokenize_and_encode_examples__(self.disagree_path, self.disagree_files)\n",
        "        self.__pre_tokenize_and_encode_examples__(self.nostance_path, self.nostance_files)\n",
        "        self.__pre_tokenize_and_encode_examples__(self.notrelevant_path, self.notrelevant_files)\n",
        "\n",
        "    def __pre_tokenize_and_encode_examples__(self, path, files):\n",
        "        \"\"\"\n",
        "        Function to tokenize & encode examples and save the tokenized versions to a separate folder.\n",
        "        This way, we won't have to perform the same tokenization and encoding ops every epoch.\n",
        "        \"\"\"\n",
        "        if not os.path.exists(os.path.join(path, 'tokenized_and_encoded')):\n",
        "            os.mkdir(os.path.join(path, 'tokenized_and_encoded'))\n",
        "\n",
        "            # Clean & tokenize tweets\n",
        "            for i in trange(len(files), desc='Tokenizing & Encoding {} Tweets'.format(path),\n",
        "                            leave=True):\n",
        "                file = files[i]\n",
        "                m_id = int(file.split('_')[0])\n",
        "                idx = target_idx.index(m_id)\n",
        "                target = targets[idx]\n",
        "                with open(os.path.join(path, file), mode='r', encoding='utf8') as f:\n",
        "                    example = f.read()\n",
        "                example = re.sub(r'<br />', '', example)\n",
        "                example = example.lstrip().rstrip()\n",
        "                example = re.sub(' +', ' ', example)\n",
        "                example = tokenize_and_encode(text=example,\n",
        "                                              header=target,\n",
        "                                              tokenizer=self.tokenizer,\n",
        "                                              apply_cleaning=self.apply_cleaning,\n",
        "                                              max_tokenization_length=self.max_tokenization_length,\n",
        "                                              truncation_method=self.truncation_method,\n",
        "                                              split_head_density=self.split_head_density)\n",
        "\n",
        "                with open(os.path.join(path, 'tokenized_and_encoded', file), mode='wb') as f:\n",
        "                    pickle.dump(obj=example, file=f)\n",
        "        else:\n",
        "            logging.warning('Tokenized {} directory already exists!'.format(path))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.agree_files) + len(self.disagree_files) + len(self.nostance_files) + len(self.notrelevant_files)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # agree\n",
        "        if index < self.num_agree_examples:\n",
        "            file = self.agree_files[index]\n",
        "            label = torch.tensor(data=self.agree_label, dtype=torch.long).to(self.device)\n",
        "            with open(os.path.join(self.agree_path, 'tokenized_and_encoded', file), mode='rb') as f:\n",
        "                example = pickle.load(file=f)\n",
        "        elif index < self.num_agree_examples + self.num_disagree_examples:\n",
        "            file = self.disagree_files[index - self.num_agree_examples]\n",
        "            label = torch.tensor(data=self.disagree_label, dtype=torch.long).to(self.device)\n",
        "            with open(os.path.join(self.disagree_path, 'tokenized_and_encoded', file), mode='rb') as f:\n",
        "                example = pickle.load(file=f)\n",
        "        elif index < self.num_agree_examples + self.num_disagree_examples + self.num_nostance_examples:\n",
        "            file = self.nostance_files[index - self.num_agree_examples - self.num_disagree_examples]\n",
        "            label = torch.tensor(data=self.nostance_label, dtype=torch.long).to(self.device)\n",
        "            with open(os.path.join(self.nostance_path, 'tokenized_and_encoded', file), mode='rb') as f:\n",
        "                example = pickle.load(file=f)\n",
        "        elif index < self.num_agree_examples + self.num_disagree_examples + self.num_nostance_examples + self.num_notrelevant_examples:\n",
        "            file = self.notrelevant_files[index - self.num_agree_examples - self.num_disagree_examples - self.num_nostance_examples]\n",
        "            label = torch.tensor(data=self.notrelevant_label, dtype=torch.long).to(self.device)\n",
        "            with open(os.path.join(self.notrelevant_path, 'tokenized_and_encoded', file), mode='rb') as f:\n",
        "                example = pickle.load(file=f)\n",
        "        else:\n",
        "            raise ValueError('Out of range index while accessing dataset')\n",
        "\n",
        "        return torch.from_numpy(np.array(example)).long().to(self.device), label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9iycmVn1g7sI",
        "outputId": "f62e875d-2f49-4979-c414-039d4a03cc70"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from pytorch_transformers import AdamW  # Adam's optimization w/ fixed weight decay\n",
        "import pickle\n",
        "\n",
        "# Initialize train & test datasets\n",
        "train_dataset = TweetDataset(input_directory=\"drive/MyDrive/CS 6320 Project/data/train\",\n",
        "                            tokenizer=model.get_tokenizer(),\n",
        "                            apply_cleaning=True,\n",
        "                            max_tokenization_length=MAX_TOKENIZATION_LENGTH,\n",
        "                            truncation_method='head-only',\n",
        "                            device=DEVICE)\n",
        "\n",
        "test_dataset = TweetDataset(input_directory=\"drive/MyDrive/CS 6320 Project/data/test\",\n",
        "                           tokenizer=model.get_tokenizer(),\n",
        "                           apply_cleaning=True,\n",
        "                           max_tokenization_length=MAX_TOKENIZATION_LENGTH,\n",
        "                           truncation_method='head-only',\n",
        "                           device=DEVICE)\n",
        "\n",
        "print(train_dataset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Tokenized drive/MyDrive/CS 6320 Project/data/train/agree directory already exists!\n",
            "WARNING:root:Tokenized drive/MyDrive/CS 6320 Project/data/train/disagree directory already exists!\n",
            "WARNING:root:Tokenized drive/MyDrive/CS 6320 Project/data/train/no_stance directory already exists!\n",
            "WARNING:root:Tokenized drive/MyDrive/CS 6320 Project/data/train/not_relevant directory already exists!\n",
            "WARNING:root:Tokenized drive/MyDrive/CS 6320 Project/data/test/agree directory already exists!\n",
            "WARNING:root:Tokenized drive/MyDrive/CS 6320 Project/data/test/disagree directory already exists!\n",
            "WARNING:root:Tokenized drive/MyDrive/CS 6320 Project/data/test/no_stance directory already exists!\n",
            "WARNING:root:Tokenized drive/MyDrive/CS 6320 Project/data/test/not_relevant directory already exists!\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "<__main__.TweetDataset object at 0x7fb522cd0790>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNUq8sYYk5ut"
      },
      "source": [
        "# Acquire iterators through data loaders\n",
        "train_loader = DataLoader(dataset=train_dataset,\n",
        "                          batch_size=BATCH_SIZE,\n",
        "                          shuffle=True)\n",
        "\n",
        "test_loader = DataLoader(dataset=test_dataset,\n",
        "                         batch_size=BATCH_SIZE,\n",
        "                         shuffle=False)\n",
        "\n",
        "# Define loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Define identifiers & group model parameters accordingly (check README.md for the intuition)\n",
        "bert_learning_rate = BERT_LEARNING_RATE\n",
        "custom_learning_rate = CUSTOM_LEARNING_RATE\n",
        "bert_identifiers = ['embeddings']\n",
        "no_weight_decay_identifiers = ['bias', 'LayerNorm.weight']\n",
        "grouped_model_parameters = [\n",
        "        {'params': [param for name, param in model.named_parameters()\n",
        "                    if any(identifier in name for identifier in bert_identifiers) and\n",
        "                    not any(identifier_ in name for identifier_ in no_weight_decay_identifiers)],\n",
        "          'lr': bert_learning_rate,\n",
        "          'betas': (0.9, 0.999),\n",
        "          'weight_decay': 0.01,\n",
        "          'eps': 1e-8},\n",
        "        {'params': [param for name, param in model.named_parameters()\n",
        "                    if any(identifier in name for identifier in bert_identifiers) and\n",
        "                    any(identifier_ in name for identifier_ in no_weight_decay_identifiers)],\n",
        "          'lr': bert_learning_rate,\n",
        "          'betas': (0.9, 0.999),\n",
        "          'weight_decay': 0.0,\n",
        "          'eps': 1e-8},\n",
        "        {'params': [param for name, param in model.named_parameters()\n",
        "                    if not any(identifier in name for identifier in bert_identifiers)],\n",
        "          'lr': custom_learning_rate,\n",
        "          'betas': (0.9, 0.999),\n",
        "          'weight_decay': 0.0,\n",
        "          'eps': 1e-8}\n",
        "]\n",
        "# Define optimizer\n",
        "optimizer = AdamW(grouped_model_parameters)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dSLq8zcwjAkU"
      },
      "source": [
        "# Place model & loss function on GPU\n",
        "model, criterion = model.to(DEVICE), criterion.to(DEVICE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BPKHP0jqjnnz"
      },
      "source": [
        "from sklearn.metrics import f1_score, confusion_matrix\n",
        "\n",
        "def binary_accuracy(y_pred, y_true):\n",
        "    \"\"\"Function to calculate binary accuracy per batch\"\"\"\n",
        "    y_pred_max = torch.argmax(y_pred, dim=-1)\n",
        "    correct_pred = (y_pred_max == y_true).float()\n",
        "    acc = correct_pred.sum() / len(correct_pred)\n",
        "    return acc\n",
        "\n",
        "\n",
        "def train(model, iterator, criterion, optimizer, device, include_bert_masks=True):\n",
        "    \"\"\"\n",
        "    Function to carry out the training process\n",
        "    @param (torch.nn.Module) model: model object to be trained\n",
        "    @param (torch.utils.data.DataLoader) iterator: data loader to iterate over batches\n",
        "    @param (torch.nn.[...]) criterion: loss function to backpropagate on\n",
        "    @param (torch.optim.[...]) optimizer: optimization algorithm\n",
        "    @param (torch.device) device: 'cpu' or 'gpu', decides where to store the outputted tensors\n",
        "    @param (bool) include_bert_masks: whether to include token type IDs & attention masks alongside\n",
        "           input IDs when passing to model or not (default: True)\n",
        "    \"\"\"\n",
        "    epoch_loss, epoch_acc = 0.0, 0.0\n",
        "\n",
        "    for batch in iterator:\n",
        "        # Get training input IDs & labels from the current batch\n",
        "        input_ids, labels = batch\n",
        "        # Get corresponding additional features from the current batch\n",
        "        token_type_ids, attention_mask = get_features(input_ids=input_ids,\n",
        "                                                      tokenizer=model.get_tokenizer(),\n",
        "                                                      device=device)\n",
        "        # Reset the gradients from previous processes\n",
        "        optimizer.zero_grad()\n",
        "        # Pass features through the model w/ or w/o BERT masks for attention & token type\n",
        "        if include_bert_masks:\n",
        "            predictions = model(input_ids=input_ids,\n",
        "                                token_type_ids=token_type_ids,\n",
        "                                attention_mask=attention_mask)\n",
        "        else:\n",
        "            predictions = model(input_ids=input_ids,header_ids=header_ids)\n",
        "\n",
        "        # Calculate loss and accuracy\n",
        "        loss = criterion(predictions, labels)\n",
        "        acc = binary_accuracy(predictions, labels)\n",
        "\n",
        "        # Backward and optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
        "\n",
        "\n",
        "def test(model, iterator, criterion, device, include_bert_masks=True):\n",
        "    \"\"\"\n",
        "    Function to carry out the testing (or validation) process\n",
        "    @param (torch.nn.Module) model: model object to be trained\n",
        "    @param (torch.utils.data.DataLoader) iterator: data loader to iterate over batches\n",
        "    @param (torch.nn.[...]) criterion: loss function to backpropagate on\n",
        "    @param (torch.device) device: 'cpu' or 'gpu', decides where to store the outputted tensors\n",
        "    @param (bool) include_bert_masks: whether to include token type IDs & attention masks alongside\n",
        "           input IDs when passing to model or not (default: True)\n",
        "    \"\"\"\n",
        "    epoch_loss, epoch_acc = 0.0, 0.0\n",
        "    confusion = [[0 for _ in range(0, 4)] for _ in range(0, 4)]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in iterator:\n",
        "            # Get testing input IDs & labels from the current batch\n",
        "            input_ids, labels = batch\n",
        "            # Get corresponding additional features from the current batch\n",
        "            token_type_ids, attention_mask = get_features(input_ids=input_ids,\n",
        "                                                          tokenizer=model.get_tokenizer(),\n",
        "                                                          device=device)\n",
        "            # Pass features through the model w/ or w/o BERT masks for attention & token type\n",
        "            if include_bert_masks:\n",
        "                predictions = model(input_ids=input_ids,\n",
        "                                    token_type_ids=token_type_ids,\n",
        "                                    attention_mask=attention_mask)\n",
        "            else:\n",
        "                predictions = model(input_ids=input_ids)\n",
        "\n",
        "            # Calculate loss and accuracy\n",
        "            loss = criterion(predictions, labels)\n",
        "            acc = binary_accuracy(predictions, labels)\n",
        "            y_pred_max = torch.argmax(predictions, dim=-1)\n",
        "            confusion += confusion_matrix(labels.cpu().detach().numpy(), \n",
        "                                          y_pred_max.cpu().detach().numpy(), \n",
        "                                          labels=[0,1,2,3])\n",
        "            \n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "\n",
        "    \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator), confusion\n",
        "\n",
        "\n",
        "def get_attention_nth_layer_mth_head_kth_token(attention_outputs, n, m, k, average_heads=False):\n",
        "    \"\"\"\n",
        "    Function to compute attention weights by:\n",
        "    i)   Take the attention weights from the nth multi-head attention layer assigned to kth token\n",
        "    ii)  Take the mth attention head\n",
        "    \"\"\"\n",
        "    if average_heads is True and m is not None:\n",
        "        logging.warning(\"Argument passed for param @m will be ignored because of head averaging.\")\n",
        "\n",
        "    # Get the attention weights outputted by the nth layer\n",
        "    attention_outputs_concatenated = torch.cat(attention_outputs, dim=0)       # (K, N, P, P)\n",
        "    attention_outputs = attention_outputs_concatenated.data[n, :, :, :]        # (N, P, P)\n",
        "\n",
        "    # Get the attention weights assigned to kth token\n",
        "    attention_outputs = attention_outputs[:, k, :]                             # (N, P)\n",
        "\n",
        "    # Compute the average attention weights across all attention heads\n",
        "    if average_heads:\n",
        "        attention_outputs = torch.sum(attention_outputs, dim=0)                # (P)\n",
        "        num_attention_heads = attention_outputs_concatenated.shape[1]\n",
        "        attention_outputs /= num_attention_heads\n",
        "    # Get the attention weights of mth head\n",
        "    else:\n",
        "        attention_outputs = attention_outputs[m, :]                            # (P)\n",
        "\n",
        "    return attention_outputs\n",
        "\n",
        "\n",
        "def get_attention_average_first_layer(attention_outputs):\n",
        "    \"\"\"\n",
        "    Function to compute attention weights by:\n",
        "    i)   Take the attention weights from the first multi-head attention layer assigned to CLS\n",
        "    ii)  Average each token across attention heads\n",
        "    \"\"\"\n",
        "    return get_attention_nth_layer_mth_head_kth_token(attention_outputs=attention_outputs,\n",
        "                                                      n=0, m=None, k=0,\n",
        "                                                      average_heads=True)\n",
        "\n",
        "\n",
        "def get_attention_average_last_layer(attention_outputs):\n",
        "    \"\"\"\n",
        "    Function to compute attention weights by\n",
        "    i)   Take the attention weights from the last multi-head attention layer assigned to CLS\n",
        "    ii)  Average each token across attention heads\n",
        "    \"\"\"\n",
        "    return get_attention_nth_layer_mth_head_kth_token(attention_outputs=attention_outputs,\n",
        "                                                      n=-1, m=None, k=0,\n",
        "                                                      average_heads=True)\n",
        "\n",
        "\n",
        "def get_normalized_attention(model, raw_sentence, method='last_layer_heads_average',\n",
        "                             n=None, m=None, k=None, exclude_special_tokens=True,\n",
        "                             normalization_method='normal', device='cpu'):\n",
        "    \"\"\"\n",
        "    Function to get the normalized version of the attention output of a FineTunedBert() model\n",
        "    @param (torch.nn.Module) model: FineTunedBert() model to visualize attention weights on\n",
        "    @param (str) raw_sentence: sentence in string format, preferably from the test distribution\n",
        "    @param (str) method: method name specifying the attention output configuration, possible values\n",
        "           are 'first_layer_heads_average', 'last_layer_heads_average', 'nth_layer_heads_average',\n",
        "           'nth_layer_mth_head', and 'custom' (default: 'last_layer_heads_average')\n",
        "    @param (int) n: layer no. (default: None)\n",
        "    @param (int) m: head no. (default: None)\n",
        "    @param (int) k: token no. (default: None)\n",
        "    @param (bool) exclude_special_tokens: whether to exclude special tokens such as [CLS] and [SEP]\n",
        "           from attention weights computation or not (default: True)\n",
        "    @param (str) normalization_method: the normalization method to be applied on attention weights,\n",
        "           possible values include 'min-max' and 'normal' (default: 'normal')\n",
        "    @param (torch.device) device: 'cpu' or 'gpu', decides where to store the outputted tensors\n",
        "    \"\"\"\n",
        "    if None in [n, m, k] and method == 'custom':\n",
        "        raise ValueError(\"Must pass integer argument for params @n, @m, and @k \" +\n",
        "                         \"if method is 'nth_layer_mth_head_kth_token'\")\n",
        "    elif None not in [n, m, k] and method != 'custom':\n",
        "        logging.warning(\"Arguments passed for params @n, @m, or @k will be ignored. \" +\n",
        "                        \"Specify @method as 'nth_layer_mth_head_kth_token' to make them effective.\")\n",
        "\n",
        "    # Plug in CLS & SEP special tokens for identification of start & end points of sequences\n",
        "    if '[CLS]' not in raw_sentence and '[SEP]' not in raw_sentence:\n",
        "        tokenized_text = ['[CLS]'] + model.get_tokenizer().tokenize(raw_sentence) + ['[SEP]']\n",
        "    else:\n",
        "        tokenized_text = model.get_tokenizer().tokenize(raw_sentence)\n",
        "\n",
        "    # Call model evaluation as we don't want no gradient update\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        attention_outputs = model.get_bert_attention(raw_sentence=raw_sentence, device=device)\n",
        "\n",
        "    attention_weights = None\n",
        "    if method == 'first_layer_heads_average':\n",
        "        attention_weights = get_attention_nth_layer_mth_head_kth_token(\n",
        "            attention_outputs=attention_outputs,\n",
        "            n=0, m=None, k=0,\n",
        "            average_heads=True\n",
        "        )\n",
        "    elif method == 'last_layer_heads_average':\n",
        "        attention_weights = get_attention_nth_layer_mth_head_kth_token(\n",
        "            attention_outputs=attention_outputs,\n",
        "            n=-1, m=None, k=0,\n",
        "            average_heads=True\n",
        "        )\n",
        "    elif method == 'nth_layer_heads_average':\n",
        "        attention_weights = get_attention_nth_layer_mth_head_kth_token(\n",
        "            attention_outputs=attention_outputs,\n",
        "            n=n, m=None, k=0,\n",
        "            average_heads=True\n",
        "        )\n",
        "    elif method == 'nth_layer_mth_head':\n",
        "        attention_weights = get_attention_nth_layer_mth_head_kth_token( \n",
        "            attention_outputs=attention_outputs,\n",
        "            n=n, m=m, k=0,\n",
        "            average_heads=False\n",
        "        )\n",
        "    elif method == 'custom':\n",
        "        attention_weights = get_attention_nth_layer_mth_head_kth_token(\n",
        "            attention_outputs=attention_outputs,\n",
        "            n=n, m=m, k=k,\n",
        "            average_heads=False\n",
        "        )\n",
        "\n",
        "    # Remove the beginning [CLS] & ending [SEP] tokens for better intuition\n",
        "    if exclude_special_tokens:\n",
        "        tokenized_text, attention_weights = tokenized_text[1:-1], attention_weights[1:-1]\n",
        "\n",
        "    # Apply normalization methods to attention weights\n",
        "    # i)  Min-Max Normalization\n",
        "    if normalization_method == 'min-max':\n",
        "        max_weight, min_weight = attention_weights.max(), attention_weights.min()\n",
        "        attention_weights = (attention_weights - min_weight) / (max_weight - min_weight)\n",
        "\n",
        "    # ii) Z-Score Normalization\n",
        "    elif normalization_method == 'normal':\n",
        "        mu, std = attention_weights.mean(), attention_weights.std()\n",
        "        attention_weights = (attention_weights - mu) / std\n",
        "\n",
        "    # Convert tensor to NumPy array\n",
        "    attention_weights = attention_weights.data\n",
        "\n",
        "    tokens_and_weights = []\n",
        "    for index, token in enumerate(tokenized_text):\n",
        "        tokens_and_weights.append((token, attention_weights[index].item()))\n",
        "\n",
        "    return tokens_and_weights\n",
        "\n",
        "\n",
        "def get_delta_attention(tokens_and_weights_pre, tokens_and_weights_post):\n",
        "    \"\"\"Function to compute the delta (change) in scaled attention weights before & after\"\"\"\n",
        "    tokens_and_weights_delta = []\n",
        "    for i, token_and_weight in enumerate(tokens_and_weights_pre):\n",
        "        token,  = token_and_weight[0],\n",
        "        assert token == tokens_and_weights_post[i][0]\n",
        "\n",
        "        pre_weight = token_and_weight[1]\n",
        "        post_weight = tokens_and_weights_post[i][1]\n",
        "\n",
        "        tokens_and_weights_delta.append((token, post_weight - pre_weight))\n",
        "\n",
        "    return tokens_and_weights_delta"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-soo6wtzW5Y"
      },
      "source": [
        "# Let's Demo!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Fu_Dgl7qKjN",
        "outputId": "6c3d5602-b5fa-4230-8d17-2195577a25f7"
      },
      "source": [
        "# Load the weights\n",
        "model.load_state_dict(torch.load('drive/MyDrive/CS 6320 Project/saved_models/BERT-b1-r1-h256-d6-blr1e-5.pt'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWd533cW69ZR"
      },
      "source": [
        "def predict(tweet, header_id, print_results = False):\n",
        "    labels = [\"agree\", \"disagree\", \"no_stance\", \"not_relevant\"]\n",
        "\n",
        "    prediction = model.predict(tweet, header_id)\n",
        "    if print_results:\n",
        "        print(\"Target: \" + targets[target_idx.index(header_id)])\n",
        "        print(\"Tweet: \" + tweet)\n",
        "        print(\"Stance: \" + labels[torch.argmax(prediction, dim=-1)])\n",
        "    return labels[torch.argmax(prediction, dim=-1)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h__9Nb41zOeb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 154
        },
        "outputId": "22254a6c-2fc2-490d-b0bb-7753e28f6450"
      },
      "source": [
        "tweet = \"@JCope222 Vaccine is untested for safety. People have died in he trials. Natural immunity scores 99.975% recovery from Covid for your age group\" \n",
        "header_id = 1\n",
        "\n",
        "tweet_demo = \"My moms doesn't want me to continue my education because we have to get a Covid vaccine and she says it's a \\\"fact\\\" that it causes infertility\"\n",
        "header_id_demo = 2\n",
        "\n",
        "predict(tweet, header_id, print_results=True)\n",
        "print(\"=\"*100)\n",
        "predict(tweet_demo, header_id_demo, print_results=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Target: RNA alters a person's DNA when taking the COVID-19 vaccine.\n",
            "Tweet: @JCope222 Vaccine is untested for safety. People have died in he trials. Natural immunity scores 99.975% recovery from Covid for your age group\n",
            "Stance: not_relevant\n",
            "====================================================================================================\n",
            "Target: The COVID-19 vaccine causes infertility or miscarriages in women.\n",
            "Tweet: My moms doesn't want me to continue my education because we have to get a Covid vaccine and she says it's a \"fact\" that it causes infertility\n",
            "Stance: disagree\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'disagree'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    }
  ]
}